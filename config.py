#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SoulChat ChatGLM2-6b LoRAå¾®è°ƒé…ç½®æ–‡ä»¶
ç»Ÿä¸€ç®¡ç†æ‰€æœ‰è®­ç»ƒå’Œæ¨ç†å‚æ•°
"""

import os
from dataclasses import dataclass
from typing import Optional, List

@dataclass
class ModelConfig:
    """æ¨¡å‹ç›¸å…³é…ç½®"""
    # åŸºç¡€æ¨¡å‹è·¯å¾„
    base_model_path: str = "models/chatglm2-6b"
    # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
    trust_remote_code: bool = True
    # æ¨¡å‹ç²¾åº¦
    torch_dtype: str = "float16"  # float16, bfloat16, float32
    # è®¾å¤‡æ˜ å°„
    device_map: str = "auto"

@dataclass
class LoRAConfig:
    """LoRAå¾®è°ƒé…ç½®"""
    # LoRA rank
    r: int = 8
    # LoRA alphaå‚æ•°
    alpha: int = 32
    # Dropoutç‡
    dropout: float = 0.1
    # ç›®æ ‡æ¨¡å—
    target_modules: List[str] = None
    # åç½®å‚æ•°
    bias: str = "none"
    # ä»»åŠ¡ç±»å‹
    task_type: str = "CAUSAL_LM"
    
    def __post_init__(self):
        if self.target_modules is None:
            # ChatGLM2-6bçš„é»˜è®¤ç›®æ ‡æ¨¡å—
            self.target_modules = [
                "query_key_value",
                "dense",
                "dense_h_to_4h", 
                "dense_4h_to_h"
            ]

@dataclass
class DataConfig:
    """æ•°æ®ç›¸å…³é…ç½®"""
    # åŸå§‹è®­ç»ƒæ•°æ®è·¯å¾„
    raw_train_data_path: str = "data/PsyDTCorpus_train_mulit_turn_packing.json"
    # åŸå§‹æµ‹è¯•æ•°æ®è·¯å¾„
    raw_test_data_path: str = "data/PsyDTCorpus_test_single_turn_split.json"
    # å¤„ç†åçš„è®­ç»ƒæ•°æ®è·¯å¾„
    processed_train_data_path: str = "data/sft_train_data.json"
    # å¤„ç†åçš„æµ‹è¯•æ•°æ®è·¯å¾„
    processed_test_data_path: str = "data/sft_test_data.json"
    # æœ€å¤§è¾“å…¥é•¿åº¦
    max_source_length: int = 512
    # æœ€å¤§è¾“å‡ºé•¿åº¦
    max_target_length: int = 512
    # æœ€å¤§åºåˆ—é•¿åº¦
    max_seq_length: int = 1024

@dataclass
class TrainingConfig:
    """è®­ç»ƒç›¸å…³é…ç½®"""
    # è¾“å‡ºç›®å½•
    output_dir: str = "output"
    # æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°
    per_device_train_batch_size: int = 1
    # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    gradient_accumulation_steps: int = 4
    # å­¦ä¹ ç‡
    learning_rate: float = 2e-5
    # è®­ç»ƒè½®æ•°
    num_train_epochs: int = 3
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler_type: str = "cosine"
    # é¢„çƒ­æ¯”ä¾‹
    warmup_ratio: float = 0.1
    # æ—¥å¿—æ­¥æ•°
    logging_steps: int = 10
    # ä¿å­˜æ­¥æ•°
    save_steps: int = 500
    # æœ€å¤§ä¿å­˜æ£€æŸ¥ç‚¹æ•°
    save_total_limit: int = 3
    # æ˜¯å¦ä½¿ç”¨fp16
    fp16: bool = True
    # æ˜¯å¦ä½¿ç”¨bf16
    bf16: bool = False
    # æ¢¯åº¦æ£€æŸ¥ç‚¹
    gradient_checkpointing: bool = True
    # æ•°æ®åŠ è½½å™¨å›ºå®šå†…å­˜
    dataloader_pin_memory: bool = False
    # ç§»é™¤æœªä½¿ç”¨çš„åˆ—
    remove_unused_columns: bool = False
    # æŠ¥å‘Šå·¥å…·
    report_to: str = "tensorboard"
    # éšæœºç§å­
    seed: int = 42

@dataclass
class InferenceConfig:
    """æ¨ç†ç›¸å…³é…ç½®"""
    # æœ€å¤§ç”Ÿæˆé•¿åº¦
    max_length: int = 2048
    # æ¸©åº¦å‚æ•°
    temperature: float = 0.7
    # Top-pé‡‡æ ·
    top_p: float = 0.8
    # Top-ké‡‡æ ·
    top_k: int = 50
    # æ˜¯å¦é‡‡æ ·
    do_sample: bool = True
    # é‡å¤æƒ©ç½š
    repetition_penalty: float = 1.1

@dataclass
class PathConfig:
    """è·¯å¾„ç›¸å…³é…ç½®"""
    # LoRAæƒé‡ä¿å­˜è·¯å¾„
    lora_weights_path: str = "output/lora_weights"
    # åˆå¹¶æ¨¡å‹ä¿å­˜è·¯å¾„
    merged_model_path: str = "output/merged_model"
    # è®­ç»ƒæ—¥å¿—è·¯å¾„
    logs_path: str = "output/logs"
    # æ£€æŸ¥ç‚¹è·¯å¾„
    checkpoint_path: str = "output/checkpoints"

class Config:
    """ä¸»é…ç½®ç±»"""
    
    def __init__(self):
        self.model = ModelConfig()
        self.lora = LoRAConfig()
        self.data = DataConfig()
        self.training = TrainingConfig()
        self.inference = InferenceConfig()
        self.paths = PathConfig()
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self._create_directories()
    
    def _create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = [
            self.training.output_dir,
            self.paths.lora_weights_path,
            self.paths.merged_model_path,
            self.paths.logs_path,
            self.paths.checkpoint_path,
            "data"
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def get_training_args_dict(self):
        """è·å–è®­ç»ƒå‚æ•°å­—å…¸"""
        return {
            "output_dir": self.training.output_dir,
            "per_device_train_batch_size": self.training.per_device_train_batch_size,
            "gradient_accumulation_steps": self.training.gradient_accumulation_steps,
            "learning_rate": self.training.learning_rate,
            "num_train_epochs": self.training.num_train_epochs,
            "lr_scheduler_type": self.training.lr_scheduler_type,
            "warmup_ratio": self.training.warmup_ratio,
            "logging_steps": self.training.logging_steps,
            "save_steps": self.training.save_steps,
            "save_total_limit": self.training.save_total_limit,
            "fp16": self.training.fp16,
            "bf16": self.training.bf16,
            "gradient_checkpointing": self.training.gradient_checkpointing,
            "dataloader_pin_memory": self.training.dataloader_pin_memory,
            "remove_unused_columns": self.training.remove_unused_columns,
            "report_to": self.training.report_to,
            "logging_dir": self.paths.logs_path,
            "seed": self.training.seed
        }
    
    def get_lora_config_dict(self):
        """è·å–LoRAé…ç½®å­—å…¸"""
        return {
            "r": self.lora.r,
            "lora_alpha": self.lora.alpha,
            "lora_dropout": self.lora.dropout,
            "target_modules": self.lora.target_modules,
            "bias": self.lora.bias,
            "task_type": self.lora.task_type
        }
    
    def print_config(self):
        """æ‰“å°é…ç½®ä¿¡æ¯"""
        print("=" * 60)
        print("SoulChat ChatGLM2-6b LoRAå¾®è°ƒé…ç½®")
        print("=" * 60)
        
        print(f"\nğŸ“‚ æ¨¡å‹é…ç½®:")
        print(f"  - åŸºç¡€æ¨¡å‹: {self.model.base_model_path}")
        print(f"  - ç²¾åº¦: {self.model.torch_dtype}")
        print(f"  - è®¾å¤‡æ˜ å°„: {self.model.device_map}")
        
        print(f"\nğŸ”§ LoRAé…ç½®:")
        print(f"  - Rank: {self.lora.r}")
        print(f"  - Alpha: {self.lora.alpha}")
        print(f"  - Dropout: {self.lora.dropout}")
        print(f"  - ç›®æ ‡æ¨¡å—: {', '.join(self.lora.target_modules)}")
        
        print(f"\nğŸ“Š æ•°æ®é…ç½®:")
        print(f"  - æœ€å¤§è¾“å…¥é•¿åº¦: {self.data.max_source_length}")
        print(f"  - æœ€å¤§è¾“å‡ºé•¿åº¦: {self.data.max_target_length}")
        print(f"  - è®­ç»ƒæ•°æ®: {self.data.processed_train_data_path}")
        
        print(f"\nğŸ¯ è®­ç»ƒé…ç½®:")
        print(f"  - æ‰¹æ¬¡å¤§å°: {self.training.per_device_train_batch_size}")
        print(f"  - æ¢¯åº¦ç´¯ç§¯: {self.training.gradient_accumulation_steps}")
        print(f"  - å­¦ä¹ ç‡: {self.training.learning_rate}")
        print(f"  - è®­ç»ƒè½®æ•°: {self.training.num_train_epochs}")
        print(f"  - ç²¾åº¦: {'fp16' if self.training.fp16 else 'fp32'}")
        
        print(f"\nğŸš€ æ¨ç†é…ç½®:")
        print(f"  - æœ€å¤§é•¿åº¦: {self.inference.max_length}")
        print(f"  - æ¸©åº¦: {self.inference.temperature}")
        print(f"  - Top-p: {self.inference.top_p}")
        
        print("=" * 60)

# å…¨å±€é…ç½®å®ä¾‹
config = Config()

# æµ‹è¯•ç”¨ä¾‹é…ç½®
TEST_CASES = [
    "æˆ‘æœ€è¿‘å¾ˆç„¦è™‘ï¼Œä¸çŸ¥é“æ€ä¹ˆåŠ",
    "æˆ‘æ„Ÿè§‰å¾ˆå­¤ç‹¬ï¼Œæ²¡æœ‰äººç†è§£æˆ‘", 
    "å·¥ä½œå‹åŠ›å¾ˆå¤§ï¼Œæ¯å¤©éƒ½å¾ˆç´¯",
    "æˆ‘å’Œæœ‹å‹åµæ¶äº†ï¼Œå¿ƒæƒ…å¾ˆä¸å¥½",
    "æˆ‘å¯¹æœªæ¥æ„Ÿåˆ°è¿·èŒ«å’Œä¸å®‰",
    "å¤±æ‹äº†ï¼Œå¿ƒé‡Œå¾ˆç—›è‹¦",
    "å­¦ä¹ æˆç»©ä¸å¥½ï¼Œçˆ¶æ¯å¾ˆå¤±æœ›",
    "æˆ‘è§‰å¾—è‡ªå·±ä¸€æ— æ˜¯å¤„",
    "æœ€è¿‘ç¡çœ è´¨é‡å¾ˆå·®",
    "æˆ‘å¾ˆæ‹…å¿ƒå®¶äººçš„å¥åº·çŠ¶å†µ"
]

if __name__ == "__main__":
    # æ‰“å°é…ç½®ä¿¡æ¯
    config.print_config() 