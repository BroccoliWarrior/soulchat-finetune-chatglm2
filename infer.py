#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ChatGLM2-6b LoRAå¾®è°ƒæ¨¡å‹æ¨ç†è„šæœ¬
å½»åº•ç»•è¿‡transformersç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
"""

import torch
from transformers import AutoTokenizer, AutoModel
from peft import PeftModel
import argparse
import os
import warnings
import sys
warnings.filterwarnings("ignore")

class UltimateSoulChatInferencer:
    """å¿ƒç†ç–å¯¼å¯¹è¯æ¨ç†å™¨"""
    
    def __init__(self, base_model_path: str, lora_weights_path: str = None, merged_model_path: str = None):
        """åˆå§‹åŒ–æ¨ç†å™¨"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        if merged_model_path and os.path.exists(merged_model_path):
            # åŠ è½½åˆå¹¶åçš„æ¨¡å‹
            print(f"åŠ è½½åˆå¹¶åçš„æ¨¡å‹: {merged_model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(
                merged_model_path,
                trust_remote_code=True,
                padding_side="left"
            )
            self.model = AutoModel.from_pretrained(
                merged_model_path,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map="auto"
            )
        else:
            # åŠ è½½åŸºç¡€æ¨¡å‹ + LoRAæƒé‡
            print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(
                base_model_path,
                trust_remote_code=True,
                padding_side="left"
            )
            self.model = AutoModel.from_pretrained(
                base_model_path,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            if lora_weights_path and os.path.exists(lora_weights_path):
                print(f"åŠ è½½LoRAæƒé‡: {lora_weights_path}")
                self.model = PeftModel.from_pretrained(self.model, lora_weights_path)
                print("åˆå¹¶LoRAæƒé‡åˆ°æ¨¡å‹...")
                self.model = self.model.merge_and_unload()
        
        self.model.eval()
        print("æ¨¡å‹åŠ è½½å®Œæˆ!")
        
        # è®¾ç½®ç‰¹æ®Štoken
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # å°è¯•ç¦ç”¨problematicæ–¹æ³•
        self._patch_model_methods()
        
        self.system_prompt = (
            "ä½ æ˜¯ä¸€ä½ç²¾é€šç†æƒ…è¡Œä¸ºç–—æ³•ï¼ˆRational Emotive Behavior Therapyï¼Œç®€ç§°REBTï¼‰çš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œ"
            "èƒ½å¤Ÿåˆç†åœ°é‡‡ç”¨ç†æƒ…è¡Œä¸ºç–—æ³•ç»™æ¥è®¿è€…æä¾›ä¸“ä¸šåœ°æŒ‡å¯¼å’Œæ”¯æŒï¼Œç¼“è§£æ¥è®¿è€…çš„è´Ÿé¢æƒ…ç»ªå’Œè¡Œä¸ºååº”ï¼Œ"
            "å¸®åŠ©ä»–ä»¬å®ç°ä¸ªäººæˆé•¿å’Œå¿ƒç†å¥åº·ã€‚\n\n"
            "ç†æƒ…è¡Œä¸ºæ²»ç–—ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š\n"
            "ï¼ˆ1ï¼‰æ£€æŸ¥éç†æ€§ä¿¡å¿µå’Œè‡ªæˆ‘æŒ«è´¥å¼æ€ç»´ï¼šå’¨è¯¢å¸ˆå¸®åŠ©æ¥è®¿è€…æ¢æŸ¥éšè—åœ¨æƒ…ç»ªå›°æ‰°åé¢çš„åŸå› ï¼Œ"
            "å¹¶åçœå…¶å†…åœ¨è‡ªæˆ‘å¯¹è¯ã€‚\n"
            "ï¼ˆ2ï¼‰ä¸éç†æ€§ä¿¡å¿µè¾©è®ºï¼šä½¿ç”¨è®¤çŸ¥æŠ€æœ¯è´¨ç–‘éç†æ€§ä¿¡å¿µçš„ä¸åˆç†æ€§ï¼Œå¼•å¯¼æ¥è®¿è€…æ„¿æ„æ”¾å¼ƒå®ƒä»¬ã€‚\n"
            "ï¼ˆ3ï¼‰å¾—å‡ºåˆç†ä¿¡å¿µï¼Œå­¦ä¼šç†æ€§æ€ç»´ï¼šååŠ©æ¥è®¿è€…æ‰¾åˆ°æ›´åˆé€‚çš„åº”å¯¹æ–¹å¼ï¼Œå¹¶é‡å¤æ•™å¯¼å…¶åˆç†æ€§ã€‚\n"
            "ï¼ˆ4ï¼‰è¿ç§»åº”ç”¨æ²»ç–—æ”¶è·ï¼šé¼“åŠ±æ¥è®¿è€…å°†æ²»ç–—ä¸­çš„æ”¶è·åº”ç”¨äºæ—¥å¸¸ç”Ÿæ´»ï¼Œå®ç°æŒç»­æˆé•¿ã€‚"
        )

    
    def _patch_model_methods(self):
        """ä¿®è¡¥æ¨¡å‹æ–¹æ³•ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜"""
        try:
            # å¦‚æœæ¨¡å‹æœ‰chatæ–¹æ³•ï¼Œä¸´æ—¶ç¦ç”¨å®ƒ
            if hasattr(self.model, 'chat'):
                self.model._original_chat = self.model.chat
                # ç”¨æˆ‘ä»¬çš„æ–¹æ³•æ›¿æ¢
                self.model.chat = self._safe_chat_wrapper
            print("å·²åº”ç”¨å…¼å®¹æ€§è¡¥ä¸")
        except Exception as e:
            print(f"åº”ç”¨è¡¥ä¸æ—¶å‡ºé”™ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
    
    def _safe_chat_wrapper(self, tokenizer, query, history=None, max_length=2048, **kwargs):
        """å®‰å…¨çš„chatæ–¹æ³•åŒ…è£…å™¨"""
        return self.chat(query), []
    
    def generate_response_direct(self, user_input: str, max_new_tokens: int = 200):
        """
        ç›´æ¥ç”Ÿæˆå›å¤ - ç»•è¿‡æ‰€æœ‰å¯èƒ½çš„å…¼å®¹æ€§é—®é¢˜
        """
        try:
            # æ„å»ºprompt
            prompt = f"é—®ï¼š{user_input}\nç­”ï¼š"
            
            # ç¼–ç è¾“å…¥
            input_ids = self.tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=True)
            input_ids = input_ids.to(self.device)
            
            # åˆ›å»ºattention mask
            attention_mask = torch.ones_like(input_ids)
            
            input_length = input_ids.shape[1]
            
            # æ‰‹åŠ¨å®ç°ç”Ÿæˆé€»è¾‘
            generated_ids = input_ids.clone()
            
            with torch.no_grad():
                for step in range(max_new_tokens):
                    # è·å–ä¸‹ä¸€ä¸ªtokençš„logits
                    outputs = self.model(
                        input_ids=generated_ids,
                        attention_mask=torch.ones_like(generated_ids),
                        use_cache=False,
                        output_attentions=False,
                        output_hidden_states=False
                    )
                    
                    # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                    next_token_logits = outputs.logits[:, -1, :]
                    
                    # åº”ç”¨temperature
                    next_token_logits = next_token_logits / 0.7
                    
                    # Top-pé‡‡æ ·
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                    sorted_indices_to_remove = cumulative_probs > 0.8
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = False
                    
                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                    next_token_logits[indices_to_remove] = -float('inf')
                    
                    # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                    probs = torch.softmax(next_token_logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                    
                    # æ·»åŠ åˆ°åºåˆ—
                    generated_ids = torch.cat([generated_ids, next_token], dim=-1)
                    
                    # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æŸtoken
                    if next_token.item() == self.tokenizer.eos_token_id:
                        break
            
            # è§£ç ç”Ÿæˆçš„éƒ¨åˆ†
            new_tokens = generated_ids[0][input_length:]
            response = self.tokenizer.decode(new_tokens, skip_special_tokens=True)
            
            # æ¸…ç†è¾“å‡º
            response = self._clean_response(response)
            
            return response
            
        except Exception as e:
            print(f"ç›´æ¥ç”Ÿæˆå¤±è´¥: {e}")
            raise e
    
    def generate_response_simple(self, user_input: str, max_new_tokens: int = 150):
        """
        ç®€åŒ–ç”Ÿæˆæ–¹æ³• - æœ€å°åŒ–å…¼å®¹æ€§é—®é¢˜
        """
        try:
            # æ›´ç®€å•çš„prompt
            prompt = user_input
            
            # ç¼–ç 
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=False, truncation=False)
            input_ids = inputs["input_ids"].to(self.device)
            
            # ç¡®ä¿æœ‰attention_mask
            if "attention_mask" in inputs:
                attention_mask = inputs["attention_mask"].to(self.device)
            else:
                attention_mask = torch.ones_like(input_ids)
            
            input_length = input_ids.shape[1]
            
            with torch.no_grad():
                # ä½¿ç”¨æœ€åŸºç¡€çš„forward pass
                for _ in range(max_new_tokens):
                    try:
                        # ç›´æ¥è°ƒç”¨æ¨¡å‹forward
                        outputs = self.model.forward(
                            input_ids=input_ids,
                            attention_mask=attention_mask
                        )
                        
                        # è·å–logits
                        logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]
                        next_token_logits = logits[0, -1, :]
                        
                        # ç®€å•çš„è´ªå©ªè§£ç 
                        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                        
                        # æ£€æŸ¥ç»“æŸæ¡ä»¶
                        if next_token_id.item() == self.tokenizer.eos_token_id:
                            break
                        
                        # æ·»åŠ æ–°token
                        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=-1)
                        attention_mask = torch.cat([attention_mask, torch.ones((1, 1), device=self.device)], dim=-1)
                        
                        # é™åˆ¶é•¿åº¦
                        if input_ids.shape[1] > input_length + max_new_tokens:
                            break
                            
                    except Exception as inner_e:
                        print(f"å†…éƒ¨ç”Ÿæˆé”™è¯¯: {inner_e}")
                        break
            
            # è§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            new_tokens = input_ids[0][input_length:]
            response = self.tokenizer.decode(new_tokens, skip_special_tokens=True)
            
            return self._clean_response(response)
            
        except Exception as e:
            print(f"ç®€åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            raise e
    
    def _clean_response(self, response: str) -> str:
        """æ¸…ç†å“åº”æ–‡æœ¬"""
        response = response.strip()
        
        # ç§»é™¤å¯èƒ½çš„æ ¼å¼æ ‡è®°
        if "ç­”ï¼š" in response:
            response = response.split("ç­”ï¼š")[-1].strip()
        if "é—®ï¼š" in response:
            response = response.split("é—®ï¼š")[0].strip()
        
        # ç§»é™¤é‡å¤çš„æ–‡æœ¬
        lines = response.split('\n')
        if len(lines) > 1:
            response = lines[0].strip()
        
        return response
    

    
    def chat(self, user_input: str):
        """ä¸»è¦å¯¹è¯æ–¹æ³•"""
        print("æ­£åœ¨ç”Ÿæˆå›å¤...", end="", flush=True)
        
        prompt_with_context = f"{self.system_prompt}\n\næ¥è®¿è€…ï¼š{user_input}\nå¿ƒç†å’¨è¯¢å¸ˆï¼š"
        
        # å°è¯•å¤šç§ç”Ÿæˆæ–¹æ³•
        methods = [
            ("ç›´æ¥ç”Ÿæˆ", self.generate_response_direct),
            ("ç®€åŒ–ç”Ÿæˆ", self.generate_response_simple)
        ]
        
        for method_name, method in methods:
            try:
                response = method(prompt_with_context)
                if response and len(response.strip()) > 0:
                    print(f"\rä½¿ç”¨äº†{method_name}")
                    return response
            except Exception as e:
                print(f"\r{method_name}å¤±è´¥: {e}")
                continue
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†ï¼ŒæŠ›å‡ºå¼‚å¸¸
        raise Exception("æ‰€æœ‰ç”Ÿæˆæ–¹æ³•éƒ½å¤±è´¥äº†")
    
    def interactive_chat(self):
        """äº¤äº’å¼å¯¹è¯"""
        print("\nğŸ¤– æ¬¢è¿ä½¿ç”¨å¿ƒç†ç–å¯¼å¯¹è¯ç³»ç»Ÿ!")
        print("ğŸ’ æˆ‘æ˜¯ä½ çš„AIå¿ƒç†å¥åº·åŠ©æ‰‹ï¼Œéšæ—¶å€¾å¬ä½ çš„å¿ƒå£°")
        print("ğŸ“ è¾“å…¥'é€€å‡º'ã€'exit'ã€'quit'æˆ–'bye'ç»“æŸå¯¹è¯")
        print("ğŸ”„ è¾“å…¥'é‡å¯'é‡æ–°å¼€å§‹å¯¹è¯")
        print("=" * 60)
        
        conversation_count = 0
        
        while True:
            try:
                user_input = input(f"\nğŸ’¬ [{conversation_count + 1}] ä½ : ").strip()
                
                if not user_input:
                    continue
                
                if user_input.lower() in ['é€€å‡º', 'exit', 'quit', 'bye']:
                    print("\nğŸŒŸ æ„Ÿè°¢ä½ çš„ä¿¡ä»»å’Œåˆ†äº«ã€‚")
                    print("ğŸ’ª è®°ä½ï¼Œå¯»æ±‚å¸®åŠ©æ˜¯å‹‡æ•¢çš„è¡¨ç°ï¼Œä½ å¹¶ä¸å­¤å•ã€‚")
                    print("ğŸ¥ å¦‚æœéœ€è¦ä¸“ä¸šå¸®åŠ©ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šå¿ƒç†å¥åº·æœºæ„ã€‚")
                    print("ğŸ‘‹ å†è§ï¼Œç¥ä½ ä¸€åˆ‡å®‰å¥½ï¼")
                    break
                
                if user_input == 'é‡å¯':
                    conversation_count = 0
                    print("ğŸ”„ å¯¹è¯å·²é‡å¯")
                    continue
                
                # ç”Ÿæˆå›å¤
                try:
                    response = self.chat(user_input)
                    print(f"ğŸ¤– åŠ©æ‰‹: {response}")
                    conversation_count += 1
                    
                    # å®šæœŸæé†’
                    if conversation_count % 5 == 0:
                        print("\nğŸ’¡ æ¸©é¦¨æé†’ï¼šæˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œå¯ä»¥æä¾›æƒ…æ„Ÿæ”¯æŒï¼Œä½†å¦‚æœé‡åˆ°ä¸¥é‡å¿ƒç†å›°æ‰°ï¼Œè¯·åŠæ—¶å¯»æ±‚ä¸“ä¸šå¿ƒç†å’¨è¯¢å¸ˆçš„å¸®åŠ©ã€‚")
                        
                except Exception as e:
                    print(f"âŒ ç”Ÿæˆå›å¤å¤±è´¥: {e}")
                    print("ğŸ”§ è¯·å°è¯•é‡æ–°è¾“å…¥æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
                
            except KeyboardInterrupt:
                print("\n\nâš¡ å¯¹è¯è¢«ä¸­æ–­ã€‚")
                print("ğŸŒˆ å¸Œæœ›æˆ‘ä»¬çš„äº¤æµå¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚ä¿é‡ï¼")
                break
            except Exception as e:
                print(f"\nâŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                print("ğŸ”„ è®©æˆ‘ä»¬ç»§ç»­å°è¯•...")
    
    def batch_test(self):
        """æ‰¹é‡æµ‹è¯•åŠŸèƒ½"""
        test_cases = [
            "æˆ‘æœ€è¿‘å‹åŠ›å¾ˆå¤§ï¼Œæ€»æ˜¯å¤±çœ ",
            "æ„Ÿè§‰å¾ˆå­¤ç‹¬ï¼Œæ²¡æœ‰äººçœŸæ­£ç†è§£æˆ‘",
            "å·¥ä½œä¸Šé‡åˆ°äº†å¾ˆå¤§çš„æŒ«æŠ˜",
            "å’Œå®¶äººçš„å…³ç³»å¾ˆç´§å¼ ",
            "å¯¹æœªæ¥æ„Ÿåˆ°éå¸¸è¿·èŒ«å’Œç„¦è™‘",
            "æœ€è¿‘æ€»æ˜¯èƒ¡æ€ä¹±æƒ³ï¼Œæ— æ³•é›†ä¸­æ³¨æ„åŠ›",
            "åˆšåˆšåˆ†æ‰‹ï¼Œå¿ƒæƒ…éå¸¸ä½è½",
            "å­¦ä¹ å‹åŠ›å¤ªå¤§ï¼Œæƒ³è¦æ”¾å¼ƒ",
            "è§‰å¾—è‡ªå·±ä¸€æ— æ˜¯å¤„ï¼Œå¾ˆè‡ªå‘",
            "æœ€è¿‘æƒ…ç»ªæ³¢åŠ¨å¾ˆå¤§ï¼Œæ§åˆ¶ä¸äº†è‡ªå·±"
        ]
        
        print("\nğŸ§ª å¼€å§‹æ‰¹é‡æµ‹è¯•...")
        print("=" * 60)
        
        success_count = 0
        
        for i, question in enumerate(test_cases, 1):
            print(f"\nğŸ“‹ æµ‹è¯• {i}/{len(test_cases)}")
            print(f"â“ é—®é¢˜: {question}")
            print("-" * 40)
            
            try:
                response = self.chat(question)
                if response and len(response.strip()) > 10:
                    print(f"âœ… å›å¤: {response}")
                    success_count += 1
                else:
                    print(f"âš ï¸  å›å¤è¿‡çŸ­: {response}")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            
            print("=" * 40)
        
        print(f"\nğŸ“Š æµ‹è¯•å®Œæˆ: {success_count}/{len(test_cases)} æˆåŠŸ")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AuraChatæ¨ç†è„šæœ¬")
    parser.add_argument("--base_model", type=str, default="models/chatglm2-6b", 
                       help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora_weights", type=str, default="output/lora_weights", 
                       help="LoRAæƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--merged_model", type=str, default="output/merged_model", 
                       help="åˆå¹¶åæ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--mode", type=str, default="interactive", 
                       choices=["interactive", "test", "single"], 
                       help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--question", type=str, default="", 
                       help="å•æ¬¡æ¨¡å¼ä¸‹çš„é—®é¢˜")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨AuraChat...")
    print(f"ğŸ¯ è¿è¡Œæ¨¡å¼: {args.mode}")
    
    try:
        # åˆå§‹åŒ–æ¨ç†å™¨
        inferencer = UltimateSoulChatInferencer(
            base_model_path=args.base_model,
            lora_weights_path=args.lora_weights if os.path.exists(args.lora_weights or "") else None,
            merged_model_path=args.merged_model if os.path.exists(args.merged_model or "") else None
        )
        
        if args.mode == "interactive":
            inferencer.interactive_chat()
        elif args.mode == "test":
            inferencer.batch_test()
        elif args.mode == "single":
            question = args.question or input("è¯·è¾“å…¥ä½ çš„é—®é¢˜: ").strip()
            if question:
                print(f"\nâ“ é—®é¢˜: {question}")
                response = inferencer.chat(question)
                print(f"ğŸ¤– å›å¤: {response}")
            else:
                print("âŒ æœªæä¾›é—®é¢˜")
        
    except Exception as e:
        print(f"ğŸ’¥ å¯åŠ¨å¤±è´¥: {e}")
        print("ğŸ”§ è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œç¯å¢ƒé…ç½®")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 